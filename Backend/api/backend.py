from flask import Flask, jsonify, request
from flask_cors import CORS
import pandas as pd
import numpy as np
import os
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime, timedelta
import joblib

app = Flask(__name__)
CORS(app)

# ==============================================================================
#  PATH CONFIGURATION
# ==============================================================================
API_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_ROOT = os.path.dirname(API_DIR)

PATHS = {
    'models': os.path.join(BACKEND_ROOT, 'models'),
    'cache': os.path.join(BACKEND_ROOT, 'cache'),
    # Keep CSV as fallback
    'prices': os.path.join(BACKEND_ROOT, 'data', 'raw', 'BrentOilPrices.csv'),
    'macro': os.path.join(BACKEND_ROOT, 'data', 'processed', 'brent_oil_enriched.csv'),
    'events': os.path.join(BACKEND_ROOT, 'results', 'change_point_results.csv')
}

# Create cache directory if it doesn't exist
os.makedirs(PATHS['cache'], exist_ok=True)

# ==============================================================================
#  DATABASE CONNECTION (PostgreSQL)
# ==============================================================================
# Connect to PostgreSQL
DB_CONFIG = {
    'host': os.getenv('DB_HOST'),
    'database': os.getenv('DB_NAME'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'port': os.getenv('DB_PORT')
}

def get_db_connection():
    """Create PostgreSQL connection"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        return conn
    except Exception as e:
        print(f"‚ùå Database connection failed: {e}")
        return None


def load_from_db(table_name):
    """Load data from PostgreSQL table"""
    conn = get_db_connection()
    if conn is None:
        return None

    try:
        query = f"SELECT * FROM {table_name} ORDER BY date"
        df = pd.read_sql(query, conn)
        conn.close()
        return df
    except Exception as e:
        print(f"‚ùå Error loading {table_name}: {e}")
        if conn:
            conn.close()
        return None


# ==============================================================================
#  LSTM MODEL LOADER
# ==============================================================================
class LSTMForecastService:
    def __init__(self):
        self.model = None
        self.price_scaler = None
        self.forecast_cache = None
        self.cache_timestamp = None
        self.load_model()
        self.load_cache()

    def load_model(self):
        """Load trained LSTM model and scalers"""
        try:
            from tensorflow.keras.models import load_model

            model_path = os.path.join(PATHS['models'], 'lstm_model.keras')
            scaler_path = os.path.join(PATHS['models'], 'price_scaler.pkl')

            if os.path.exists(model_path):
                self.model = load_model(model_path)
                print("‚úÖ LSTM model loaded")
            else:
                print("‚ö†Ô∏è LSTM model not found at", model_path)

            if os.path.exists(scaler_path):
                self.price_scaler = joblib.load(scaler_path)
                print("‚úÖ Price scaler loaded")
            else:
                print("‚ö†Ô∏è Price scaler not found")

        except Exception as e:
            print(f"‚ùå Error loading LSTM model: {e}")

    def load_cache(self):
        """Load pre-computed forecast from cache"""
        cache_file = os.path.join(PATHS['cache'], 'lstm_forecast.json')

        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r') as f:
                    data = json.load(f)
                    self.forecast_cache = data['forecast']
                    self.cache_timestamp = data['timestamp']
                    print(f"‚úÖ Loaded cached forecast from {self.cache_timestamp}")
            except Exception as e:
                print(f"‚ùå Error loading cache: {e}")

    def get_forecast(self):
        """Get forecast from cache (pre-computed)"""
        # Check if cache is fresh (less than 24 hours old)
        if self.forecast_cache and self.cache_timestamp:
            cache_time = datetime.fromisoformat(self.cache_timestamp)
            if datetime.now() - cache_time < timedelta(hours=24):
                return self.forecast_cache

        # If cache is stale or missing, return error
        # (Forecast should be regenerated by background job)
        return None


# Initialize service
lstm_service = LSTMForecastService()


# ==============================================================================
#  SMART COLUMN MAPPER (Keep for CSV fallback)
# ==============================================================================
def resolve_columns(df):
    """Scans DataFrame and maps column names"""
    cols = df.columns
    mapping = {}

    candidates_map = {
        'Event': ['Event', 'event', 'Event_Name', 'EventName', 'events'],
        'Date': ['Detected_Date', 'Date', 'Change_Point_Date', 'Change_Point', 'date'],
        'Pre': ['Pre_Event_Vol', 'Pre_Vol', 'sigma_1', 'Sigma_1', 'Vol_1', 'pre_vol'],
        'Post': ['Post_Event_Vol', 'Post_Vol', 'sigma_2', 'Sigma_2', 'Vol_2', 'post_vol'],
        'Pct': ['Volatility_Change_Pct', 'Pct_Change', 'Change_Pct', 'impact']
    }

    for key, candidates in candidates_map.items():
        for c in candidates:
            if c in cols:
                mapping[key] = c
                break
        if key not in mapping and key == 'Event':
            mapping[key] = cols[0]  # Fallback

    return mapping

# ============================================================
#  FIX 1: NaN / Infinity in JSON
#  Replace your existing load_dataset() function with this
# ============================================================

def load_dataset(key):
    """Load from PostgreSQL or CSV fallback - with strict NaN cleaning"""
    table_mapping = {
        'prices': 'market_data',
        'events': 'detected_change_points',
        'macro':  'market_data'
    }

    if key in table_mapping:
        df = load_from_db(table_mapping[key])
        if df is not None:
            if 'date' in df.columns:
                df['Date'] = pd.to_datetime(df['date'])
            # ‚Üê FIX: replace ALL non-JSON-safe values
            df = df.replace([np.nan, np.inf, -np.inf], None)
            return df

    path = PATHS.get(key)
    if not path or not os.path.exists(path):
        print(f"‚ùå ERROR: File not found at: {path}")
        return None
    try:
        df = pd.read_csv(path)
        if 'Date' in df.columns:
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df = df.dropna(subset=['Date']).sort_values('Date')
        # ‚Üê FIX: replace ALL non-JSON-safe values
        df = df.replace([np.nan, np.inf, -np.inf], None)
        return df
    except Exception as e:
        print(f"‚ùå ERROR reading {key}: {e}")
        return None



# ==============================================================================
#  EXISTING ENDPOINTS (Keep as-is)
# ==============================================================================

@app.route('/api/model/regimes', methods=['GET'])
def get_regime_history():
    """Original endpoint - unchanged"""
    df_prices = load_dataset('prices')
    df_events = load_dataset('events')

    if df_prices is None:
        return jsonify({"error": "Price data missing"}), 404

    df_prices['regime'] = 'Normal'
    df_prices['regime_volatility'] = 0.02

    if df_events is not None:
        col_map = resolve_columns(df_events)

        if 'Pre' in col_map and 'Post' in col_map:
            for _, event in df_events.iterrows():
                try:
                    det_date = pd.to_datetime(event[col_map.get('Date', 'Detected_Date')])
                    start = det_date - pd.Timedelta(days=365)
                    end = det_date + pd.Timedelta(days=365)

                    if 'Window_Start' in df_events.columns:
                        start = pd.to_datetime(event['Window_Start'])
                    if 'Window_End' in df_events.columns:
                        end = pd.to_datetime(event['Window_End'])

                    pre_vol = float(event[col_map['Pre']])
                    post_vol = float(event[col_map['Post']])

                    is_high_vol = post_vol > pre_vol
                    regime_label = 'High_Vol' if is_high_vol else 'Low_Vol'

                    mask = (df_prices['Date'] >= start) & (df_prices['Date'] <= end)
                    df_prices.loc[mask, 'regime'] = regime_label

                    mask_pre = mask & (df_prices['Date'] < det_date)
                    mask_post = mask & (df_prices['Date'] >= det_date)

                    df_prices.loc[mask_pre, 'regime_volatility'] = pre_vol
                    df_prices.loc[mask_post, 'regime_volatility'] = post_vol
                except Exception as e:
                    continue

    start_arg = request.args.get('start')
    end_arg = request.args.get('end')
    if start_arg and end_arg:
        df_prices = df_prices[(df_prices['Date'] >= start_arg) & (df_prices['Date'] <= end_arg)]

    if len(df_prices) > 5000:
        df_prices = df_prices.iloc[::2]

    return jsonify(df_prices.to_dict(orient='records'))


@app.route('/api/model/event/<event_name>', methods=['GET'])
def get_event_distributions(event_name):
    """Original endpoint - unchanged"""
    df_events = load_dataset('events')
    if df_events is None:
        return jsonify({"error": "Events missing"}), 404

    col_map = resolve_columns(df_events)
    event_col = col_map['Event']

    try:
        mask = df_events[event_col].astype(str).str.lower() == event_name.lower()
        event_row = df_events[mask]
    except:
        return jsonify({"error": "Search failed"}), 500

    if event_row.empty:
        return jsonify({"error": "Event not found"}), 404

    event = event_row.iloc[0]

    pre_vol = float(event[col_map['Pre']]) if 'Pre' in col_map else 0.01
    post_vol = float(event[col_map['Post']]) if 'Post' in col_map else 0.02
    pct_change = event[col_map['Pct']] if 'Pct' in col_map else 0

    response = {
        "event_name": event[event_col],
        "detected_date": event[col_map.get('Date', 'Detected_Date')],
        "confidence": event.get('Confidence_Interval', 0.94),
        "distributions": {
            "pre": {"std": pre_vol, "label": "Stable Regime"},
            "post": {"std": post_vol, "label": "New Regime"}
        },
        "stats": {
            "volatility_change_pct": pct_change
        }
    }
    return jsonify(response)

@app.route('/api/macro/correlations', methods=['GET'])
def get_rolling_correlations():
    df = load_dataset('macro')
    if df is None:
        return jsonify({"error": "Macro data missing"}), 404

    # ‚Üê FIX: detect the actual price column name (case-insensitive)
    price_col = next(
        (c for c in df.columns if c.lower() == 'price'),
        None
    )
    usd_col = next(
        (c for c in df.columns if c.lower() in ('usd_index', 'usdindex', 'usd')),
        None
    )

    if price_col is None or usd_col is None:
        return jsonify({
            "error": f"Required columns not found. Available: {list(df.columns)}"
        }), 500

    df['corr_usd'] = df[price_col].rolling(90).corr(df[usd_col])
    df_clean = df.dropna(subset=['corr_usd']).tail(2000)
    df_clean = df_clean.replace([np.nan, np.inf, -np.inf], None)

    date_col = 'Date' if 'Date' in df_clean.columns else 'date'

    return jsonify(
        df_clean[[date_col, price_col, 'corr_usd']]
        .rename(columns={price_col: 'Price', date_col: 'Date'})
        .to_dict(orient='records')
    )



@app.route('/api/events', methods=['GET'])
def get_event_list():
    """Original endpoint - unchanged"""
    df = load_dataset('events')
    if df is None:
        return jsonify([]), 200

    col_map = resolve_columns(df)

    output = []
    for i, row in df.iterrows():
        item = {
            'id': i + 1,
            'Event': row[col_map['Event']],
            'Detected_Date': row[col_map.get('Date', 'Detected_Date')],
            'Volatility_Change_Pct': row[col_map['Pct']] if 'Pct' in col_map else 0,
            'Pre_Event_Vol': row[col_map['Pre']] if 'Pre' in col_map else 0,
            'Post_Event_Vol': row[col_map['Post']] if 'Post' in col_map else 0,
            'Confidence_Interval': row.get('Confidence_Interval', 0.94)
        }
        output.append(item)

    return jsonify(output)


# ==============================================================================
#  NEW LSTM ENDPOINTS
# ==============================================================================

@app.route('/api/forecast/lstm', methods=['GET'])
def get_lstm_forecast():
    """
    Returns 6-month LSTM price forecast with confidence bands
    Pre-computed and cached for performance
    """
    forecast = lstm_service.get_forecast()

    if forecast is None:
        return jsonify({
            "error": "Forecast cache not available",
            "message": "Please run forecast generation script"
        }), 503

    return jsonify(forecast)


@app.route('/api/forecast/scenarios', methods=['GET'])
def get_forecast_scenarios():
    """
    Returns multiple forecast scenarios:
    - Base case (LSTM)
    - Optimistic (low volatility)
    - Pessimistic (crisis scenario)
    """
    base_forecast = lstm_service.get_forecast()

    if base_forecast is None:
        return jsonify({"error": "Forecast not available"}), 503
    baseline_forecast = base_forecast.get('baseline', {})
    predictions = baseline_forecast.get('predictions', [])

    # Load regime data for scenario analysis
    conn = get_db_connection()
    if conn is None:
        return jsonify({"error": "Database unavailable"}), 503

    try:
        # Get detected change points for crisis statistics
        CRISIS_STATS_QUERY = """
            SELECT
                AVG(volatility_crisis / volatility_pre) AS avg_crisis_intensity,
                AVG(
                    EXTRACT(DAY FROM (
                        crisis_end_date::timestamp - crisis_start_date::timestamp
                    ))
                ) AS avg_crisis_duration
            FROM detected_change_points
        """

        cursor = conn.cursor(cursor_factory=RealDictCursor)
        cursor.execute(CRISIS_STATS_QUERY)
        crisis_stats = cursor.fetchone()
        conn.close()

        avg_intensity = crisis_stats['avg_crisis_intensity'] or 1.5

        # Generate scenarios
        scenarios = {
            "base_case": base_forecast,
            "optimistic": {
                "description": "Normal regime continues, low volatility",
                "probability": 0.70,
                "forecast": []
            },
            "pessimistic": {
                "description": "Crisis event, high volatility",
                "probability": 0.15,
                "forecast": []
            },
            "extreme": {
                "description": "Major crisis (historical worst case)",
                "probability": 0.05,
                "forecast": []
            }
        }

        # Adjust forecasts based on scenarios
        for point in predictions:
            date = point['date']
            base_price = point['predicted_price']

            # Optimistic: ¬±5% bands
            scenarios['optimistic']['forecast'].append({
                'date': date,
                'predicted_price': base_price,
                'lower_bound': base_price * 0.95,
                'upper_bound': base_price * 1.05
            })

            # Pessimistic: ¬±15% bands (crisis volatility)
            scenarios['pessimistic']['forecast'].append({
                'date': date,
                'predicted_price': base_price,
                'lower_bound': base_price * 0.85,
                'upper_bound': base_price * 1.15
            })

            # Extreme: ¬±25% bands (black swan)
            scenarios['extreme']['forecast'].append({
                'date': date,
                'predicted_price': base_price,
                'lower_bound': base_price * 0.75,
                'upper_bound': base_price * 1.25
            })

        return jsonify(scenarios)

    except Exception as e:
        print(f"‚ùå Error generating scenarios: {e}")
        if conn:
            conn.close()
        return jsonify({"error": str(e)}), 500

@app.route('/api/forecast/analysis', methods=['GET'])
def get_combined_analysis():
    forecast_payload = lstm_service.get_forecast()
    if forecast_payload is None:
        return jsonify({"error": "Forecast not available"}), 503

    # ‚Üê FIX: read from 'baseline' key (new dual-model cache structure)
    baseline = forecast_payload.get('baseline', {})
    predictions = baseline.get('predictions', [])

    if not predictions:
        return jsonify({"error": "No predictions in cache"}), 503

    conn = get_db_connection()
    if conn is None:
        return jsonify({"error": "Database unavailable"}), 503

    try:
        from psycopg2.extras import RealDictCursor

        cursor = conn.cursor(cursor_factory=RealDictCursor)

        # Current regime
        cursor.execute("""
            SELECT regime, volatility, confidence, date
            FROM regimes
            ORDER BY date DESC
            LIMIT 1
        """)
        current_regime = cursor.fetchone()

        # Days since last crisis
        cursor.execute("""
            SELECT MAX(crisis_end_date) AS last_crisis_end
            FROM detected_change_points
        """)
        last_crisis = cursor.fetchone()
        conn.close()

        days_since_crisis = 0
        if last_crisis and last_crisis['last_crisis_end']:
            days_since_crisis = (datetime.now().date() - last_crisis['last_crisis_end']).days

        # Adjust confidence based on regime
        base_confidence = forecast_payload.get('metadata', {}).get('model_confidence', 85)
        regime_name = current_regime['regime'] if current_regime else 'Normal'

        multiplier = {'Crisis': 0.6, 'Recovery': 0.8}.get(regime_name, 1.0)
        adjusted_confidence = base_confidence * multiplier

        # Recommendations
        recommendations = []
        if regime_name == 'Crisis':
            recommendations.append({
                "type": "alert",
                "message": "Crisis regime detected. Forecast confidence reduced.",
                "action": "Reduce exposure by 20-30%, increase cash reserves"
            })
        elif regime_name == 'Normal' and days_since_crisis > 180:
            recommendations.append({
                "type": "warning",
                "message": f"Stable for {days_since_crisis} days. Increased volatility risk.",
                "action": "Consider hedging strategies"
            })
        else:
            recommendations.append({
                "type": "info",
                "message": "Market in stable regime. Forecast reliable.",
                "action": "Maintain current allocation"
            })

        # Safe access to first/last prediction
        current_price = predictions[0].get('actual_price') or baseline.get('end_price', 0)
        end_price     = predictions[-1].get('predicted_price', 0)
        expected_return = ((end_price / current_price) - 1) * 100 if current_price else 0

        analysis = {
            "current_state": {
                "regime":            regime_name,
                "volatility":        current_regime['volatility'] if current_regime else 0.02,
                "confidence":        current_regime['confidence'] if current_regime else 0.85,
                "days_since_crisis": days_since_crisis,
                "date":              current_regime['date'].isoformat() if current_regime else None
            },
            "forecast": {
                "horizon_days":        180,
                "end_price":           end_price,
                "current_price":       current_price,
                "expected_return_pct": expected_return,
                "confidence":          adjusted_confidence
            },
            "risk_assessment": {
                "level":                   "High" if regime_name == 'Crisis' else "Low",
                "volatility_expectation":  "Elevated" if regime_name == 'Crisis' else "Stable",
                "crisis_probability":      0.25 if regime_name == 'Crisis' else 0.05
            },
            "recommendations": recommendations,
            "timestamp": datetime.now().isoformat()
        }

        return jsonify(analysis)

    except Exception as e:
        print(f"‚ùå Error in combined analysis: {e}")
        if conn:
            conn.close()
        return jsonify({"error": str(e)}), 500


@app.route('/api/forecast/refresh', methods=['POST'])
def refresh_forecast():
    """
    Trigger forecast regeneration (admin only)
    This would normally be called by a cron job
    """
    # TODO: Add authentication

    try:
        # This would trigger the forecast generation script
        # For now, just reload cache
        lstm_service.load_cache()

        return jsonify({
            "status": "success",
            "message": "Forecast cache reloaded",
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500


if __name__ == '__main__':
    print(f"üöÄ Enhanced API Running")
    print(f"   Backend Root: {BACKEND_ROOT}")
    print(f"   LSTM Model: {'‚úÖ Loaded' if lstm_service.model else '‚ùå Not found'}")
    print(f"   Forecast Cache: {'‚úÖ Loaded' if lstm_service.forecast_cache else '‚ùå Empty'}")
    app.run(debug=True, port=5000)
